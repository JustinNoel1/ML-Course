---
title: "Problem Session 5"
author: "Justin Noel"
date: "11/23/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)
```

## R Lab
Write a function in R that will take in a vector of discrete variables and will produce the corresponding one hot encodings.

```{r}
library(tidyverse)
#Function takes in data and returns a function which performs one hot encoding with respect to that data
OHE = function(x) {
  #Get list of unique entries (sort for convenience)
  values = unique(sort(x))
  num_values = length(values)
  encoder = function(z){
    #find index
    ix = match(z,values)
    #initialize zero vector
    v = numeric(num_values)
    v[ix]=1
    return(v)
  }
  
  #vectorized encoder
  function(y) {
    matrix(map(y,encoder))
  }
}
```

Test this on some data:
```{r}
Y = c(1,2,3)
encoder = OHE(Y)
A=encoder(Y)
A[,]

Z = c('c','e','e','a','b')
encoder2 = OHE(Z)
B = encoder2(Z)
B[,]
```

Write an LDA classifier from scratch.
```{r}
LDAClassifier = function(x,y) {
  num_features = dim(x)[2]
  num_samples = dim(x)[1]
  
  #build df
  t = as.tibble(cbind(x,y))
  colnames(t)=c(paste0('x',1:num_features), 'y')
  
  #calculate group means
  means = t %>%
    group_by(y) %>%
    summarize_all(.funs = c(mean))
  num_classes = dim(means)[1]
  
  #calculate covariance matrix
  cov_mat = matrix(numeric(num_features*num_features),num_features,num_features)
  for(ix in 1:num_samples) {
    #get vector
    v = as.matrix(t[ix,1:num_features])
    #center it
    v = v-as.matrix(means[t[[ix,(num_features+1)]],2:(num_features+1)])
    cov_mat = cov_mat + t(v) %*% v
  }
  cov_mat = cov_mat / (num_samples-num_features)
  prec_mat = solve(cov_mat)
  denominator = (det(2*pi*cov_mat))^(0.5)
  
  #only do this for single classes
  probs = function(x) {
    ve = matrix(x,ncol=num_features,nrow = 1)
    prbs = numeric(num_classes)
    for(c in 1:num_classes) {
      mu = as.matrix(means[c,2:(num_features+1)], ncol = num_features, nrow = 1)
      vec = ve-mu
      exponent = - vec %*% prec_mat %*% t(vec) / 2
      prbs[c]=exp(exponent)/denominator
    }
    return(prbs)
  }
  
  prediction = function(x) {
    which(probs(x)==max(probs(x)))
  }
  
  return(list(probs = probs, prediction = prediction, means = means[,2:(num_features+1)], covariance = cov_mat))
}
```

Let's test this on some data.

First build a data generator:
```{r}
#need multivariate Gaussian
library(MASS)
generate_lda_clusters = function(classes, dim, num_samples) {
  # generate a non-degenerate covariance matrix
  repeat{
    A = matrix(runif(dim^2),dim,dim)  
    cov_mat = A%*%t(A)
    if(det(cov_mat)!=0)
      break
  }
  # generate means
  means = matrix(1.5*runif(dim*classes),dim,classes)
  
  #initialize data frame
  xvals = matrix(numeric(dim*classes*num_samples), num_samples*classes, dim)
  y = matrix(numeric(classes*num_samples),classes*num_samples, 1)
  t = as.tibble(cbind(xvals,y))
  colnames(t)=c(paste0('x',1:dim),'y')
  
  # Fill data frame
  for(ix in 0:(num_samples*classes-1)) {
      c = (ix %/% num_samples)+1
      t[(ix+1),]=c(mvrnorm(1,means[,c],cov_mat),c)
  }
  # Generate output
  output = list(data = t, means = t(means), covariance = cov_mat)
  return(output)
}
```

Now let's check on our data:
```{r}
df = generate_lda_clusters(3,4,100)
xvals = df$data[,1:4]
yvals = df$data[,5]
ldac = LDAClassifier(xvals,yvals)
```
Compare means
```{r}
df$means
```
```{r}
ldac$means
```
```{r}
ldac$means-df$means
```

That looks okay. Although ramping up the number of samples would help confirm this.

How about the covariance matrices:
```{r}
df$covariance
```

```{r}
ldac$covariance
```
```{r}
df$covariance-ldac$covariance
```

Okay. Let's test the predictions.
```{r}
preds = apply(xvals, 1, ldac$prediction)
mean(preds == yvals)
```

Let's train the model on a subset of the data now.
```{r}
df = generate_lda_clusters(4,5,400)
xvals = df$data[,1:5]
yvals = df$data[,6]
#shuffle our data
num_samples = dim(xvals)[1]
perm = sample(1:num_samples)
xvals = xvals[perm,]
yvals = yvals[perm,]

#split it into training and test sets
split = (num_samples * 9) %/% 10
tr_xvals = xvals[1:split,]
test_xvals = xvals[(split+1):num_samples, ]
tr_yvals = yvals[1:split,]
test_yvals = yvals[(split+1):num_samples,]

#Fit our classifier
ldac = LDAClassifier(tr_xvals, tr_yvals)
preds = apply(test_xvals, 1, ldac$prediction)
mean(preds == test_yvals)
```

Okay, onto QDA:
```{r}
QDAClassifier = function(x,y) {
  num_features = dim(x)[2]
  num_samples = dim(x)[1]
  
  #build df
  t = as.tibble(cbind(x,y))
  colnames(t)=c(paste0('x',1:num_features), 'y')
  
  #calculate group means
  means = t %>%
    group_by(y) %>%
    summarize_all(.funs = c(mean))
  num_classes = dim(means)[1]
  
  #calculate covariance matrices
  cov_mats = array(numeric(num_classes*num_features^2), dim = c(num_classes, num_features, num_features))
  prec_mats = array(dim = c(num_classes, num_features, num_features))
  denominators = numeric(num_classes)
  for(c in 1:num_classes) {
    #get the samples in the class
    class_samples = t %>%
      filter(y == c) %>%
      dplyr::select(-y)
    
    num_in_class = dim(class_samples)[1]
    
    for(ix in 1:num_in_class) {
      #get vector
      v = as.matrix(class_samples[ix,])
      #center it
      v = v-as.matrix(means[c,2:(num_features+1)])
      cov_mats[c,,] = cov_mats[c,,] + t(v) %*% v
      
    }
    cov_mats[c,,] = cov_mats[c,,] / (num_in_class-1)
    prec_mats[c,,] = solve(cov_mats[c,,])
    denominators[c] = (det(2*pi*cov_mats[c,,]))^(0.5)
  }
  
  
  #only do this for single classes
  probs = function(x) {
    ve = matrix(x,ncol=num_features,nrow = 1)
    prbs = numeric(num_classes)
    for(c in 1:num_classes) {
      mu = as.matrix(means[c,2:(num_features+1)], ncol = num_features, nrow = 1)
      vec = ve-mu
      exponent = - vec %*% prec_mats[c,,] %*% t(vec) / 2
      prbs[c]=exp(exponent)/denominators[c]
    }
    return(prbs)
  }
  
  prediction = function(x) {
    which(probs(x)==max(probs(x)))
  }
  
  return(list(probs = probs, prediction = prediction, means = means[,2:(num_features+1)], covariance = cov_mats))
}
```

Generate synthetic data:
```{r}
generate_qda_clusters = function(classes, dim, num_samples) {

  # generate non-degenerate covariance matrices
  # initialize matrices
  cov_mats = array(dim = c(classes, dim, dim))
  for(c in 1:classes) {
    repeat{
      A = matrix(runif(dim^2),dim,dim)  
      cov_mat = A%*%t(A)
      if(det(cov_mat)!=0) {
        cov_mats[c,,]=cov_mat
        break
      }
    }
  }
  # generate means
  means = matrix(1.5*runif(dim*classes),dim,classes)
  
  #initialize data frame
  xvals = matrix(numeric(dim*classes*num_samples), num_samples*classes, dim)
  y = matrix(numeric(classes*num_samples),classes*num_samples, 1)
  t = as.tibble(cbind(xvals,y))
  colnames(t)=c(paste0('x',1:dim),'y')
  
  # Fill data frame
  for(ix in 0:(num_samples*classes-1)) {
      c = (ix %/% num_samples)+1
      t[(ix+1),]=c(mvrnorm(1,means[,c],cov_mats[c,,]),c)
  }
  # Generate output
  output = list(data = t, means = t(means), covariance = cov_mats)
  return(output)
}
```

Let's see if that worked:
```{r}
df = generate_qda_clusters(3,4,100)
xvals = df$data[,1:4]
yvals = df$data[,5]
qdac = QDAClassifier(xvals,yvals)
```

Compare means:
```{r}
qdac$means
```

```{r}
df$means
```
```{r}
mean((qdac$means - df$means)^2)
```

Okay covariances:
```{r}
qdac$covariance
```
```{r}
df$covariance
```
```{r}
mean((qdac$covariance-df$covariance)^2)
```

Great those estimates look okay. How are the predictions:
```{r}
df = generate_qda_clusters(2,5,400)
xvals = df$data[,1:5]
yvals = df$data[,6]
#shuffle our data
num_samples = dim(xvals)[1]
perm = sample(1:num_samples)
xvals = xvals[perm,]
yvals = yvals[perm,]

#split it into training and test sets
split = (num_samples * 9) %/% 10
tr_xvals = xvals[1:split,]
test_xvals = xvals[(split+1):num_samples, ]
tr_yvals = yvals[1:split,]
test_yvals = yvals[(split+1):num_samples,]

#Fit our classifier
qdac = QDAClassifier(tr_xvals, tr_yvals)
tr_preds = as.matrix(apply(tr_xvals, 1, qdac$prediction))
test_preds = as.matrix(apply(test_xvals,1,qdac$prediction))

# Compare with LDA
ldac = LDAClassifier(tr_xvals, tr_yvals)
tr_preds2 = as.matrix(apply(tr_xvals, 1, ldac$prediction))
test_preds2 = as.matrix(apply(test_xvals,1,ldac$prediction))
```

Let's see how we did
```{r}
output = tibble(LDA = c(mean(tr_preds2 == tr_yvals), mean(test_preds2 == test_yvals)), QDA = c(mean(tr_preds == tr_yvals), mean(test_preds == test_yvals)))
output
```

Digits
```{r}
library(keras)
mnist = dataset_mnist()
to_sample = 1:10000
x_train = mnist$train$x
x_train = array_reshape(x_train, c(nrow(x_train),784))
pca = prcomp(x_train[to_sample,], rank.=30)
tt = pca$x
y_train = mnist$train$y+1
ldac = LDAClassifier(tt, y_train[to_sample])
qdac = QDAClassifier(tt, y_train[to_sample])
tr_preds = apply(tt, 1, ldac$prediction)
tr_preds2 = apply(tt, 1, qdac$prediction)
lda_tr = mean(tr_preds == y_train[to_sample])
qda_tr = mean(tr_preds2 == y_train[to_sample])
```
Try this on test data
```{r}
to_test = 1:10000
x_test = mnist$test$x
x_test = array_reshape(x_test, c(nrow(x_test),784))
ttt = predict(pca, x_test[to_test,])
y_test = mnist$test$y+1
test_preds = apply(ttt, 1, ldac$prediction)
test_preds2 = apply(ttt, 1, qdac$prediction)
lda_test = mean(test_preds == y_test[to_test])
qda_test = mean(test_preds2 == y_test[to_test])
output = tibble(LDA = c(lda_tr, lda_test), QDA = c(qda_tr, qda_test))
output
```

