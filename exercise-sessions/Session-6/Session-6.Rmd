---
title: "Problem Session 6"
author: "Justin Noel"
date: "12/7/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F, message = F)
```

## R Lab

Here we will update our LDA classifier from last time to explicitly use the dimensionality reduction that is implicit in the underlying algorithm. 

Now we will load some of our libraries:
```{r}
library(tidyverse)
#Used for sampling from multivariate normal distribution
library(MASS)
#Used for OOP
library(R6)
```


Since we are revisiting our old code anyway. Let's first beef it up and make it into an R6 object. Objects combine data and functions, allow us to control what parts of the interface we want the user to be able to interact with, and can be organized into a hierarchy to reduce redundant code.

We want to also transform our data to lie in an appropriate subspace of the $K-1$-plane containing the $K$ centroids of our clusters. Our desired $M$ dimensional subspace comes from inductively chosing 

```{r}
LDAClassifier = function(x,y,dimension) {
  num_features = dim(x)[2]
  num_samples = dim(x)[1]
  
  #build df
  t = as.tibble(cbind(x,y))
  colnames(t)=c(paste0('x',1:num_features), 'y')
  
  #calculate group means
  means = t %>%
    group_by(y) %>%
    summarize_all(.funs = c(mean))
  class_counts = t %>% 
    group_by(y) %>% 
    summarize(n = n())
  
  num_classes = dim(means)[1]
  class_densities = class_counts[,"n"]/num_samples
  
  #calculate covariance matrix
  cov_mat = matrix(numeric(num_features*num_features),num_features,num_features)
  for(ix in 1:num_samples) {
    #get vector
    v = as.matrix(t[ix,1:num_features])
    #center it
    v = v-as.matrix(means[t[[ix,(num_features+1)]],2:(num_features+1)])
    cov_mat = cov_mat + t(v) %*% v
  }
  cov_mat = cov_mat / (num_samples-num_features)
  prec_mat = solve(cov_mat)
  denominator = (det(2*pi*cov_mat))^(0.5)
  
  #only do this for single classes
  probs = function(x) {
    ve = matrix(x,ncol=num_features,nrow = 1)
    prbs = numeric(num_classes)
    for(c in 1:num_classes) {
      mu = as.matrix(means[c,2:(num_features+1)], ncol = num_features, nrow = 1)
      vec = ve-mu
      exponent = - vec %*% prec_mat %*% t(vec) / 2
      prbs[c]=class_densities[c,"n"]*exp(exponent)/denominator
    }
    return(prbs/sum(prbs))
  }
  
  prediction = function(x) {
    which(probs(x)==max(probs(x)))
  }
  
  environment()
  return(list(probs = probs, prediction = prediction, means = means[,2:(num_features+1)], covariance = cov_mat))
}
```


Data generator:
```{r}
generate_lda_clusters = function(classes, dim, num_samples) {
  # generate a non-degenerate covariance matrix
  repeat{
    A = matrix(runif(dim^2),dim,dim)  
    cov_mat = A%*%t(A)
    if(det(cov_mat)!=0)
      break
  }
  # generate means
  means = matrix(1.5*runif(dim*classes),dim,classes)
  
  #initialize data frame
  xvals = matrix(numeric(dim*classes*num_samples), num_samples*classes, dim)
  y = matrix(numeric(classes*num_samples),classes*num_samples, 1)
  t = as.tibble(cbind(xvals,y))
  colnames(t)=c(paste0('x',1:dim),'y')
  
  # Fill data frame
  for(ix in 0:(num_samples*classes-1)) {
      c = (ix %/% num_samples)+1
      t[(ix+1),]=c(mvrnorm(1,means[,c],cov_mat),c)
  }
  # Generate output
  output = list(data = t, means = t(means), covariance = cov_mat)
  return(output)
}
```

Generate our data:
```{r}
df = generate_lda_clusters(3,4,100)
xvals = df$data[,1:4]
yvals = df$data[,5]
ldac = LDA_Classifier$new(xvals,yvals)
```

Test:
```{r}
df$means
```
```{r}
ldac$mean
```
```{r}
mean((ldac$mean-df$means)^2)
```
```{r}
preds = apply(xvals, 1, ldac$prediction)
mean(preds == yvals)
```

